<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.10"/>
<title>CvWorks: DetectorEvaluator&lt; TImg, TObj1, TObj2 &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="C3.jpg"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CvWorks
   &#160;<span id="projectnumber">0.4</span>
   </div>
   <div id="projectbrief">Computer Vision Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.10 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="examples.html"><span>Examples</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('class_vision_core_1_1_evaluation_1_1_detector_evaluator.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="class_vision_core_1_1_evaluation_1_1_detector_evaluator-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">DetectorEvaluator&lt; TImg, TObj1, TObj2 &gt; Class Template Reference<div class="ingroups"><a class="el" href="group___core.html">Core</a> &raquo; <a class="el" href="group___evaluation.html">Evaluation</a></div></div>  </div>
</div><!--header-->
<div class="contents">

<p>Provides algorithms for performance evaluation of detectors.  
 <a href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="_vision_evaluation_8h_source.html">VisionEvaluation.h</a>&gt;</code></p>
<div class="dynheader">
Collaboration diagram for DetectorEvaluator&lt; TImg, TObj1, TObj2 &gt;:</div>
<div class="dyncontent">
<div class="center"><img src="class_vision_core_1_1_evaluation_1_1_detector_evaluator__coll__graph.png" border="0" usemap="#_detector_evaluator_3_01_t_img_00_01_t_obj1_00_01_t_obj2_01_4_coll__map" alt="Collaboration graph"/></div>
<map name="_detector_evaluator_3_01_t_img_00_01_t_obj1_00_01_t_obj2_01_4_coll__map" id="_detector_evaluator_3_01_t_img_00_01_t_obj1_00_01_t_obj2_01_4_coll__map">
</map>
</div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a8e476da5933065942abba08cfd26a600"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a8e476da5933065942abba08cfd26a600"></a>
&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a8e476da5933065942abba08cfd26a600">DetectorEvaluator</a> ()</td></tr>
<tr class="memdesc:a8e476da5933065942abba08cfd26a600"><td class="mdescLeft">&#160;</td><td class="mdescRight">Default Constructor. <br /></td></tr>
<tr class="separator:a8e476da5933065942abba08cfd26a600"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae9293fe1f4d8ce45ed0d0a00d4df678f"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ae9293fe1f4d8ce45ed0d0a00d4df678f"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#ae9293fe1f4d8ce45ed0d0a00d4df678f">addTestPoint</a> (std::vector&lt; TObj1 &gt; &amp;detResult, std::vector&lt; TObj2 &gt; &amp;groundTruth)</td></tr>
<tr class="memdesc:ae9293fe1f4d8ce45ed0d0a00d4df678f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Add a detection-groundtruth pair. <br /></td></tr>
<tr class="separator:ae9293fe1f4d8ce45ed0d0a00d4df678f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0c973d5fd9480a0cca418ac96f79cf39"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a0c973d5fd9480a0cca418ac96f79cf39"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a0c973d5fd9480a0cca418ac96f79cf39">setSimilarityFunction</a> (std::function&lt; double(TObj1 &amp;, TObj2 &amp;)&gt; similarityFcn)</td></tr>
<tr class="memdesc:a0c973d5fd9480a0cca418ac96f79cf39"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set a similarity function between a detection and ground truth annotation. If the similarity function between a detected region and a ground truth annotation is greater than the threshold, it will be considered a true positive. <br /></td></tr>
<tr class="separator:a0c973d5fd9480a0cca418ac96f79cf39"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2494e14c6a1e5bc544aa5e26816267d1"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a2494e14c6a1e5bc544aa5e26816267d1"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a2494e14c6a1e5bc544aa5e26816267d1">setThreshold</a> (const double t)</td></tr>
<tr class="memdesc:a2494e14c6a1e5bc544aa5e26816267d1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the similarity threshold to decide if two objects are the same. If the similarity function between a detected region and a ground truth annotation is greater than the threshold, it will be considered a true positive. <br /></td></tr>
<tr class="separator:a2494e14c6a1e5bc544aa5e26816267d1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6f39bcdce62583279c364d901a8ab932"><td class="memItemLeft" align="right" valign="top"><a class="el" href="struct_vision_core_1_1_evaluation_1_1_detector_eval_result.html">DetectorEvalResult</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a6f39bcdce62583279c364d901a8ab932">computeResult</a> ()</td></tr>
<tr class="memdesc:a6f39bcdce62583279c364d901a8ab932"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the evaluation metrics using the entries provided via 'addTestPoint' method. Using this 'computeResult' the evaluator considers a one to one relation, using the Hungarian Algorithm to find the optimal associations. For instance, that is the same approach which has been used in "FDDB - A Benchmark for Face Detection in Unconstrained Settings" - Jain, 2010.  <a href="#a6f39bcdce62583279c364d901a8ab932">More...</a><br /></td></tr>
<tr class="separator:a6f39bcdce62583279c364d901a8ab932"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0bd934c42ffd9daa7b57142d2fa9129"><td class="memItemLeft" align="right" valign="top"><a class="el" href="struct_vision_core_1_1_evaluation_1_1_detector_eval_result.html">DetectorEvalResult</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#af0bd934c42ffd9daa7b57142d2fa9129">computeResultManyToMany</a> ()</td></tr>
<tr class="memdesc:af0bd934c42ffd9daa7b57142d2fa9129"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the evaluation metrics using the entries provided via 'addTestPoint' method. Using this 'computeResult' the evaluator considers a N-N relation where one ground truth annotation can be associated with more than one detection and vice-versa. Considering a many to many to many relation may be useful when you can precisely define where a object starts or ends. For instance, that is the same approach as in "Sun database Large-scale scene recognition from abbey to zoo" - Xiao, 2010.  <a href="#af0bd934c42ffd9daa7b57142d2fa9129">More...</a><br /></td></tr>
<tr class="separator:af0bd934c42ffd9daa7b57142d2fa9129"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affd22efb035f7daffbc8ce1ae4b1e85f"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="affd22efb035f7daffbc8ce1ae4b1e85f"></a>
<a class="el" href="struct_vision_core_1_1_evaluation_1_1_detector_eval_result.html">DetectorEvalResult</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#affd22efb035f7daffbc8ce1ae4b1e85f">evaluateDetector</a> (const <a class="el" href="class_vision_core_1_1_interfaces_1_1_detector.html">Detector</a>&lt; TImg, TObj1 &gt; &amp;det, <a class="el" href="class_vision_core_1_1_evaluation_1_1_detection_dataset.html">DetectionDataset</a>&lt; TImg, TObj2 &gt; &amp;dataset, std::function&lt; double(TObj1 &amp;, TObj2 &amp;)&gt; match)</td></tr>
<tr class="memdesc:affd22efb035f7daffbc8ce1ae4b1e85f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Runs a detector over each dataset image and compute the performance statistics, printing the result in the default output. <br /></td></tr>
<tr class="separator:affd22efb035f7daffbc8ce1ae4b1e85f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ecff40990b902c608d4df4a5dab0bd3"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a4ecff40990b902c608d4df4a5dab0bd3"></a>
<a class="el" href="struct_vision_core_1_1_evaluation_1_1_detector_eval_result.html">DetectorEvalResult</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a4ecff40990b902c608d4df4a5dab0bd3">evaluateDetectorManyToMany</a> (const <a class="el" href="class_vision_core_1_1_interfaces_1_1_detector.html">Detector</a>&lt; TImg, TObj1 &gt; &amp;det, <a class="el" href="class_vision_core_1_1_evaluation_1_1_detection_dataset.html">DetectionDataset</a>&lt; TImg, TObj2 &gt; &amp;dataset, std::function&lt; double(TObj1 &amp;, TObj2 &amp;)&gt; match)</td></tr>
<tr class="memdesc:a4ecff40990b902c608d4df4a5dab0bd3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Runs a detector over each dataset image and compute the performance statistics based on a many-to-many matching criteria. Prints the result in the default output. <br /></td></tr>
<tr class="separator:a4ecff40990b902c608d4df4a5dab0bd3"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr class="memitem:a844fc6a9b1c7fd622bb0ee308bf6d519"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a844fc6a9b1c7fd622bb0ee308bf6d519"></a>
static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a844fc6a9b1c7fd622bb0ee308bf6d519">printReport</a> (const <a class="el" href="struct_vision_core_1_1_evaluation_1_1_detector_eval_result.html">DetectorEvalResult</a> &amp;result, std::ostream &amp;out=std::cout)</td></tr>
<tr class="memdesc:a844fc6a9b1c7fd622bb0ee308bf6d519"><td class="mdescLeft">&#160;</td><td class="mdescRight">Imprime um relatório básico dos resultados. <br /></td></tr>
<tr class="separator:a844fc6a9b1c7fd622bb0ee308bf6d519"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3533b3afc49439cbb196506cffe867f8"><td class="memItemLeft" align="right" valign="top">static std::vector&lt; int &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a3533b3afc49439cbb196506cffe867f8">computeAssignmentOptimal</a> (double *W, int sizeD, int sizeGT)</td></tr>
<tr class="memdesc:a3533b3afc49439cbb196506cffe867f8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Runs the Hungarian algorithm to solve the optimal assignment problem.  <a href="#a3533b3afc49439cbb196506cffe867f8">More...</a><br /></td></tr>
<tr class="separator:a3533b3afc49439cbb196506cffe867f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;class TImg, class TObj1, class TObj2 = TObj1&gt;<br />
class VisionCore::Evaluation::DetectorEvaluator&lt; TImg, TObj1, TObj2 &gt;</h3>

<p>Provides algorithms for performance evaluation of detectors. </p>
<p>There are two forms of running the evaluation of a detector:</p>
<p>1) Manual: For each image in a test set, call a detector to get a vector of detected objects. Next, you should get the respective ground-truth for the same image. Add the pair of detected objects and ground-truth to the evaluator calling the function 'addTestPoint'. After repeating this procedure for all images in the test set, call function 'computeResults' to get the evaluation statistics. A report can be printed calling 'printReport'.</p>
<p>2) Automatic: Use function 'evaluateDetector' to evaluate a detector (object of class Detector) over a dataset (object of class <a class="el" href="class_vision_core_1_1_evaluation_1_1_detection_dataset.html" title="Abstract class for a generic detection dataset, contining images and its ground-truth. ">DetectionDataset</a>). This method will execute the steps in the manual mode for all images in the dataset.</p>
<p>Before executing the evaluation, it is necessary to define a similarity function by calling 'setSimilarityFunction'. This function computes a similarity score between a detected object and a ground-truth object.</p>
<p>TODO: add examples</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">TImg</td><td>Image type. </td></tr>
    <tr><td class="paramname">TObj1</td><td>Type returned by the detector. </td></tr>
    <tr><td class="paramname">TObj2</td><td>Type for the ground-truth. Usually - and by default - it is the same of TObj1, but could be different. </td></tr>
  </table>
  </dd>
</dl>

<p>Definition at line <a class="el" href="_vision_evaluation_8h_source.html#l00373">373</a> of file <a class="el" href="_vision_evaluation_8h_source.html">VisionEvaluation.h</a>.</p>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="a3533b3afc49439cbb196506cffe867f8"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">static std::vector&lt;int&gt; computeAssignmentOptimal </td>
          <td>(</td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>sizeD</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>sizeGT</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Runs the Hungarian algorithm to solve the optimal assignment problem. </p>
<p>The input matrix W stores the similarities between detections (rows) and ground-truths (collumns).</p>
<p>The resulting vector A contains the ground-truth indexes assigned to detections.</p>
<p>For example A=[2 0 -1 -1] means that detection 0 (index) has been assigned to ground-truth 2, detection 1 to ground-truth 0 and detection 2 and 3 have not been assigned to any ground-truth (false positive). Note that ground-truth 1 was not assigned to any detection (false negative). </p>

<p>Definition at line <a class="el" href="_vision_evaluation_8h_source.html#l00689">689</a> of file <a class="el" href="_vision_evaluation_8h_source.html">VisionEvaluation.h</a>.</p>

<p><div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a3533b3afc49439cbb196506cffe867f8_icgraph.png" border="0" usemap="#class_vision_core_1_1_evaluation_1_1_detector_evaluator_a3533b3afc49439cbb196506cffe867f8_icgraph" alt=""/></div>
<map name="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a3533b3afc49439cbb196506cffe867f8_icgraph" id="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a3533b3afc49439cbb196506cffe867f8_icgraph">
<area shape="rect" id="node2" href="class_vision_core_1_1_abstractions_1_1_detector_based_multi_tracker.html#af1a54610c9af7fb54d00e4be2e75609e" title="Given an image (i.e. video frame), update the tracked object. " alt="" coords="240,5,427,61"/>
<area shape="rect" id="node3" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a6f39bcdce62583279c364d901a8ab932" title="Compute the evaluation metrics using the entries provided via &#39;addTestPoint&#39; method. Using this &#39;computeResult&#39; the evaluator considers a one to one relation, using the Hungarian Algorithm to find the optimal associations. For instance, that is the same approach which has been used in &quot;FDDB &#45; A Benchmark for Face Detection in Unconstrained Settings&quot; &#45; Jain, 2010. " alt="" coords="279,85,388,112"/>
<area shape="rect" id="node4" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#affd22efb035f7daffbc8ce1ae4b1e85f" title="Runs a detector over each dataset image and compute the performance statistics, printing the result i..." alt="" coords="526,56,645,83"/>
<area shape="rect" id="node5" href="class_vision_core_1_1_evaluation_1_1_tracker_evaluator.html#a806aa368bb6cb66f3bc990c145456cd2" title="Compute performance statistics. " alt="" coords="475,107,696,149"/>
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="a6f39bcdce62583279c364d901a8ab932"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="struct_vision_core_1_1_evaluation_1_1_detector_eval_result.html">DetectorEvalResult</a> computeResult </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Compute the evaluation metrics using the entries provided via 'addTestPoint' method. Using this 'computeResult' the evaluator considers a one to one relation, using the Hungarian Algorithm to find the optimal associations. For instance, that is the same approach which has been used in "FDDB - A Benchmark for Face Detection in Unconstrained Settings" - Jain, 2010. </p>
<p>Computed statistics: Precision / PPV = relevant intersection retrieved / retrieved (precision is the fraction of retrieved documents that are relevant to the find)</p>
<p>Recall/ sensitivity/ Hit rate/ TPR = relevant intersection retrieved / relevant (percent of all relevant) Due the fact that we work with a 1-to-N cardinality we had to adapt the equation from TPR = TP / (TP + FN) to TPR = (GT - FN)/GT that is quite the same but written in another way</p>
<p>F1 score is the harmonic mean of precision and sensitivity</p>
<p>False discovery rate (FDR)</p>
<p>False negative rate (FNR)</p>
<p>Mean similarity </p>

<p>Definition at line <a class="el" href="_vision_evaluation_8h_source.html#l00422">422</a> of file <a class="el" href="_vision_evaluation_8h_source.html">VisionEvaluation.h</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_cgraph.png" border="0" usemap="#class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_cgraph" alt=""/></div>
<map name="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_cgraph" id="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_cgraph">
<area shape="rect" id="node2" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a3533b3afc49439cbb196506cffe867f8" title="Runs the Hungarian algorithm to solve the optimal assignment problem. " alt="" coords="163,5,349,32"/>
</map>
</div>
</p>

<p><div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_icgraph.png" border="0" usemap="#class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_icgraph" alt=""/></div>
<map name="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_icgraph" id="class_vision_core_1_1_evaluation_1_1_detector_evaluator_a6f39bcdce62583279c364d901a8ab932_icgraph">
<area shape="rect" id="node2" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#affd22efb035f7daffbc8ce1ae4b1e85f" title="Runs a detector over each dataset image and compute the performance statistics, printing the result i..." alt="" coords="214,5,333,32"/>
<area shape="rect" id="node3" href="class_vision_core_1_1_evaluation_1_1_tracker_evaluator.html#a806aa368bb6cb66f3bc990c145456cd2" title="Compute performance statistics. " alt="" coords="163,57,384,98"/>
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="af0bd934c42ffd9daa7b57142d2fa9129"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="struct_vision_core_1_1_evaluation_1_1_detector_eval_result.html">DetectorEvalResult</a> computeResultManyToMany </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Compute the evaluation metrics using the entries provided via 'addTestPoint' method. Using this 'computeResult' the evaluator considers a N-N relation where one ground truth annotation can be associated with more than one detection and vice-versa. Considering a many to many to many relation may be useful when you can precisely define where a object starts or ends. For instance, that is the same approach as in "Sun database Large-scale scene recognition from abbey to zoo" - Xiao, 2010. </p>
<p>For each ground truth</p>
<p>Computed statistics: Precision / PPV = relevant intersection retrieved / retrieved (precision is the fraction of retrieved documents that are relevant to the find)</p>
<p>Recall/ sensitivity/ Hit rate/ TPR = relevant intersection retrieved / relevant (percent of all relevant) Due the fact that we work with a 1-to-N cardinality we had to adapt the equation from TPR = TP / (TP + FN) to TPR = (GT - FN)/GT that is quite the same but written in another way</p>
<p>F1 score is the harmonic mean of precision and sensitivity</p>
<p>False discovery rate (FDR)</p>
<p>False negative rate (FNR)</p>
<p>Mean similarity </p>

<p>Definition at line <a class="el" href="_vision_evaluation_8h_source.html#l00515">515</a> of file <a class="el" href="_vision_evaluation_8h_source.html">VisionEvaluation.h</a>.</p>

<p><div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_vision_core_1_1_evaluation_1_1_detector_evaluator_af0bd934c42ffd9daa7b57142d2fa9129_icgraph.png" border="0" usemap="#class_vision_core_1_1_evaluation_1_1_detector_evaluator_af0bd934c42ffd9daa7b57142d2fa9129_icgraph" alt=""/></div>
<map name="class_vision_core_1_1_evaluation_1_1_detector_evaluator_af0bd934c42ffd9daa7b57142d2fa9129_icgraph" id="class_vision_core_1_1_evaluation_1_1_detector_evaluator_af0bd934c42ffd9daa7b57142d2fa9129_icgraph">
<area shape="rect" id="node2" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html#a4ecff40990b902c608d4df4a5dab0bd3" title="Runs a detector over each dataset image and compute the performance statistics based on a many&#45;to&#45;man..." alt="" coords="240,5,436,32"/>
</map>
</div>
</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>D:/FURG/Software/CvWorksRelease1/Core/Vision/<a class="el" href="_vision_evaluation_8h_source.html">VisionEvaluation.h</a></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespace_vision_core.html">VisionCore</a></li><li class="navelem"><a class="el" href="namespace_vision_core_1_1_evaluation.html">Evaluation</a></li><li class="navelem"><a class="el" href="class_vision_core_1_1_evaluation_1_1_detector_evaluator.html">DetectorEvaluator</a></li>
    <li class="footer">Generated on Wed Nov 18 2015 11:12:12 for CvWorks by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.10 </li>
  </ul>
</div>
</body>
</html>
